{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82141c1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-17a85a571b39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sys.path.append(\"common\")\n",
    "from sagemaker import get_execution_role#, wait_for_s3_object\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "\n",
    "# install gym environments if needed\n",
    "!pip install gym\n",
    "!pip install env_utils\n",
    "#from env_utils import VectoredGymEnvironment\n",
    "\n",
    "# S3 bucket\n",
    "sage_session = sagemaker.session.Session()\n",
    "s3_bucket = sage_session.default_bucket()\n",
    "region_name = sage_session.boto_region_name\n",
    "s3_output_path = \"s3://{}/\".format(s3_bucket)  # SDK appends the job name and output folder\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))\n",
    "\n",
    "# create unique job name\n",
    "job_name_prefix = \"rl-blackjack\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bcca8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run in local mode?\n",
    "local_mode = False\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except:\n",
    "    role = get_execution_role()\n",
    "\n",
    "print(\"Using IAM role arn: {}\".format(role))\n",
    "\n",
    "# only run from SageMaker notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_mode:\n",
    "    !/bin/bash ./common/setup.sh\n",
    "\n",
    "! pip install -U gym\n",
    "! pip install -U torch\n",
    "! pip install gym[toy_text]\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "env.observation_space\n",
    "\n",
    "env.action_space\n",
    "\n",
    "env.reset(seed=42)\n",
    "\n",
    "def get_state_idxs(state):\n",
    "    idx1, idx2, idx3 = state\n",
    "    idx3 = int(idx3)\n",
    "    return idx1, idx2, idx3\n",
    "\n",
    "def update_qtable(qtable, state, action, reward, next_state, alpha, gamma):\n",
    "    curr_idx1, curr_idx2, curr_idx3 = get_state_idxs(state)\n",
    "    next_idx1, next_idx2, next_idx3 = get_state_idxs(next_state)\n",
    "    curr_state_q = qtable[curr_idx1][curr_idx2][curr_idx3]\n",
    "    next_state_q = qtable[next_idx1][next_idx2][next_idx3]\n",
    "    qtable[curr_idx1][curr_idx2][curr_idx3][action] += \\\n",
    "            alpha * (reward + gamma * np.max(next_state_q) - curr_state_q[action])\n",
    "    return qtable\n",
    "\n",
    "def get_action(qtable, state, epsilon):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        idx1, idx2, idx3 = get_state_idxs(state)\n",
    "        action = np.argmax(qtable[idx1][idx2][idx3])\n",
    "    return action\n",
    "\n",
    "def train_agent(env,\n",
    "                qtable: np.ndarray,\n",
    "                num_episodes: int,\n",
    "                alpha: float, \n",
    "                gamma: float, \n",
    "                epsilon: float, \n",
    "                epsilon_decay: float) -> np.ndarray:\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()                                     # Added blank for extra returned argument\n",
    "        done = False\n",
    "        while True:\n",
    "            action = get_action(qtable, state, epsilon)\n",
    "            new_state, reward, done, _, info = env.step(action)    # Added blank for extra returned argument\n",
    "            qtable = update_qtable(qtable, state, action, reward, new_state, alpha, gamma)\n",
    "            state = new_state\n",
    "            if done:\n",
    "                break\n",
    "        epsilon = np.exp(-epsilon_decay*episode)\n",
    "    return qtable\n",
    "\n",
    "FIGSIZE = (8,4)\n",
    "\n",
    "def watch_trained_agent(env, qtable, num_rounds):\n",
    "    #envdisplay = JupyterDisplay(figsize=FIGSIZE)\n",
    "    rewards = []\n",
    "    for s in range(1, num_rounds+1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        round_rewards = 0\n",
    "        while True:\n",
    "            action = get_action(qtable, state, epsilon)          \n",
    "            new_state, reward, done, _, info = env.step(action)  # Added blank for extra returned argument\n",
    "            #envdisplay.show(env)\n",
    "\n",
    "            round_rewards += reward\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "        rewards.append(round_rewards)\n",
    "    return rewards\n",
    "\n",
    "FIGSIZE = (8,4)\n",
    "\n",
    "def watch_trained_agent_no_exploration(env, qtable, num_rounds):\n",
    "    #envdisplay = JupyterDisplay(figsize=FIGSIZE)\n",
    "    rewards = []\n",
    "    for s in range(1, num_rounds+1):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        round_rewards = 0\n",
    "        while True:\n",
    "            action = get_action(qtable, state, 0)                # epsilon set to 0\n",
    "            new_state, reward, done, _, info = env.step(action)  # Added blank for extra returned argument\n",
    "            #envdisplay.show(env)\n",
    "\n",
    "            round_rewards += reward\n",
    "            state = new_state\n",
    "            if done == True:\n",
    "                break\n",
    "        rewards.append(round_rewards)\n",
    "    return rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0cdab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cec8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(qtable):\n",
    "    print('PC DC Soft Pol')\n",
    "    dim1, dim2, dim3, dim4 = qtable.shape\n",
    "    for player_count in range(10,21):\n",
    "        for dealer_card in range(dim2):\n",
    "            for soft in range(dim3):\n",
    "                q_stay = qtable[player_count, dealer_card, soft, 0]\n",
    "                q_hit  = qtable[player_count, dealer_card, soft, 1]\n",
    "                pol = \"Stay\" if q_stay>=q_hit else \"Hit\"\n",
    "                print(player_count+1, dealer_card+1, soft, pol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6448e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\")\n",
    "env.action_space.seed(42)\n",
    "\n",
    "# get initial state\n",
    "state = env.reset()\n",
    "\n",
    "state_size = [x.n for x in env.observation_space]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "qtable = np.zeros(state_size + [action_size]) #init with zeros\n",
    "\n",
    "\n",
    "alpha = 0.3 # learning rate\n",
    "gamma = 0.1 # discount rate\n",
    "epsilon = 0.9     # probability that our agent will explore\n",
    "decay_rate = 0.005\n",
    "\n",
    "# training variables\n",
    "num_hands = 500_000\n",
    "\n",
    "qtable = train_agent(env,\n",
    "                     qtable,\n",
    "                     num_hands,\n",
    "                     alpha,\n",
    "                     gamma,\n",
    "                     epsilon,\n",
    "                     decay_rate)\n",
    "\n",
    "print(f\"Qtable Max: {np.max(qtable)}\")\n",
    "print(f\"Qtable Mean: {np.mean(qtable)}\")\n",
    "print(f\"Qtable Num Unique Vals: {len(np.unique(qtable))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a654c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc9855f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qt = pd.DataFrame(qtable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
